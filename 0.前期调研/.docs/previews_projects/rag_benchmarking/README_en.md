# RAG Benchmarking System

A comprehensive benchmarking system for evaluating Retrieval-Augmented Generation (RAG) systems in the medical domain.

## ğŸ¯ Project Goals

- **Standardized RAG Evaluation**: Create a unified benchmarking framework for medical RAG systems
- **Medical Domain Focus**: Specialized evaluation for Chinese medical applications
- **Open Source**: Build a community-driven standard for RAG evaluation
- **Comprehensive Metrics**: Multi-dimensional assessment of RAG performance

## ğŸ—ï¸ Architecture

```
rag_benchmarking/
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ retrieval/         # Retrieval evaluation components
â”‚   â”œâ”€â”€ generation/        # Generation evaluation components  
â”‚   â”œâ”€â”€ evaluation/        # Core evaluation logic
â”‚   â”œâ”€â”€ datasets/          # Dataset management
â”‚   â”œâ”€â”€ metrics/           # Evaluation metrics
â”‚   â”œâ”€â”€ visualization/     # Results visualization
â”‚   â””â”€â”€ reports/           # Report generation
â”œâ”€â”€ tests/                 # Test suites
â”œâ”€â”€ data/                  # Data storage
â”‚   â”œâ”€â”€ datasets/          # Test datasets
â”‚   â”œâ”€â”€ cache/             # Cached results
â”‚   â””â”€â”€ results/           # Evaluation results
â”œâ”€â”€ docs/                  # Documentation
â”œâ”€â”€ configs/               # Configuration files
â””â”€â”€ examples/              # Usage examples
```

## ğŸš€ Quick Start

### Installation

```bash
cd rag_benchmarking
pip install -e .
```

### Basic Usage

```python
from rag_benchmarking import RAGEvaluator

# Initialize evaluator
evaluator = RAGEvaluator()

# Run evaluation
results = evaluator.evaluate(
    rag_system="your_rag_system",
    dataset="medical_qa",
    metrics=["retrieval", "generation", "end_to_end"]
)

# Generate report
report = evaluator.generate_report(results)
```

## ğŸ“Š Evaluation Dimensions

### Retrieval Evaluation
- **Precision@K**: Precision at top K retrieved documents
- **Recall@K**: Recall at top K retrieved documents
- **MRR**: Mean Reciprocal Rank
- **NDCG**: Normalized Discounted Cumulative Gain
- **Latency**: Retrieval time performance

### Generation Evaluation
- **Answer Relevance**: How well answers address questions
- **Faithfulness**: Factual consistency with retrieved context
- **Completeness**: Coverage of relevant information
- **Medical Accuracy**: Domain-specific correctness
- **Hallucination Detection**: Identification of fabricated information

### End-to-End Evaluation
- **User Satisfaction**: Overall quality assessment
- **Context Utilization**: Effective use of retrieved information
- **Clinical Safety**: Safety for medical applications
- **Professionalism**: Medical professional standards

## ğŸ¥ Medical Domain Features

- **Chinese Medical Data**: Specialized for Chinese medical literature
- **Clinical Guidelines**: Evaluation based on medical standards
- **Multi-granularity**: From sentence-level to document-level assessment
- **Specialized Metrics**: Medical-specific evaluation criteria

## ğŸ”§ Configuration

Configuration files are located in `configs/`:

- `evaluation.yaml`: Evaluation parameters
- `datasets.yaml`: Dataset configurations
- `metrics.yaml`: Metric definitions
- `models.yaml`: Model configurations

## ğŸ“ˆ Reporting

- **HTML Reports**: Interactive visual reports
- **JSON Results**: Machine-readable results
- **PDF Export**: Publication-ready reports
- **API Access**: Programmatic access to results

## ğŸ¤ Contributing

We welcome contributions from the community! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- XXB Community for supporting this initiative
- Medical domain experts for evaluation guidelines
- Open source RAG frameworks for inspiration