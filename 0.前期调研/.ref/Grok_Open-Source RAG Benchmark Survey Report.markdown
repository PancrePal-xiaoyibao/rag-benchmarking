# 开源RAG基准测试项目调研报告

## 总体概述

当前开源RAG（检索增强生成）评测领域呈现出多样化的发展态势，涌现出多个成熟且社区活跃的项目。这些项目致力于为RAG系统提供标准化的评估框架和指标，以帮助开发者和研究人员准确评估和优化其RAG应用。主要流派包括ragas、FlashRAG、open-rag-eval和giskard等，它们各自在评估维度、指标设计和社区支持方面具有独特优势，为RAG技术的进步提供了坚实的基础。这些框架不仅提供了全面的评估工具，还通过灵活的架构和社区驱动的开发模式，支持多语言和特定领域的适配，适合作为构建中文医疗RAG基准测试框架的参考。

## 项目详细分析

以下是对四个最成熟、社区活跃的开源RAG评测框架的详细分析，涵盖其核心特性、评估指标、中文及领域支持情况，以及与中文医疗RAG-bench项目的对比。

### ragas

- **一句话总结**：ragas 是一个开源工具包，专门用于评估和优化大型语言模型（LLM）应用，特别是检索增强生成（RAG）系统，提供客观指标、测试数据生成以及与流行框架的无缝集成。
- **GitHub链接**：https://github.com/explodinggradients/ragas
- **官方文档**：https://docs.ragas.io/en/latest/
- **核心技术论文**：无明确核心论文，但相关讨论见 https://arxiv.org/abs/2405.07437
- **核心特性**：
  1. **客观指标**：使用基于LLM和传统的指标（如BLEU、ROUGE）精确评估RAG系统的性能。
  2. **测试数据生成**：自动生成覆盖多种场景的综合测试数据集，减少人工标注需求。
  3. **无缝集成**：与LangChain、LlamaIndex等流行LLM框架兼容，支持生产环境中的可观察性工具。
  4. **反馈循环**：利用生产数据持续改进RAG应用，优化性能。
- **评估指标**：
  - **检索指标**：上下文精度（Context Precision，衡量检索内容的准确性）、上下文召回（Context Recall，衡量检索内容的完整性）。
  - **生成指标**：忠实度（Faithfulness，评估生成内容是否忠实地反映检索内容）、答案相关性（Answer Relevance，评估答案与查询的相关性）、答案正确性（Answer Correctness，评估答案的准确性）。
  - **端到端指标**：综合评估RAG系统的整体性能，通常结合上述指标。
- **中文/领域支持**：
  - **语言支持**：ragas作为通用框架，支持多种语言，具体取决于所使用的LLM（如支持中文的模型）和数据集。中文支持可通过使用中文预训练模型（如中文BERT）实现。
  - **领域支持**：通过加载医疗领域的测试数据集（如中文医学文献或电子病历）和微调模型，可适配医疗领域。
- **简要评述**：ragas 提供了一套全面的RAG评估指标，其自动化测试数据生成和与流行框架的集成使其成为评估中文医疗RAG-bench的理想工具。其灵活性允许开发者通过配置中文模型和医疗数据集来适配特定需求，但可能需要额外的本地化工作以确保医疗术语的准确性。相比之下，ragas的社区活跃度和文档完善性使其易于上手和扩展。

### FlashRAG

- **一句话总结**：FlashRAG 是一个Python工具包，旨在重现和开发检索增强生成（RAG）研究，提供36个预处理过的基准RAG数据集和支持23种最先进的RAG算法，包括基于推理的方法。
- **GitHub链接**：https://github.com/RUC-NLPIR/FlashRAG
- **官方文档**：https://github.com/RUC-NLPIR/FlashRAG#documentation
- **核心技术论文**：https://arxiv.org/abs/2405.13576
- **核心特性**：
  1. **广泛的数据集支持**：包含36个预处理过的RAG基准数据集，覆盖多种任务类型。
  2. **算法多样性**：支持23种最先进的RAG算法，包括7种基于推理的方法，适合复杂任务。
  3. **灵活的管道组装**：允许用户根据需求灵活配置RAG管道。
  4. **用户友好的界面**：提供易于使用的UI，便于实验和调试。
  5. **多模态支持**：支持多模态RAG，包括文本、图像等数据类型。
- **评估指标**：
  - **检索指标**：如精度（Precision）、召回（Recall）、平均倒数排名（MRR）。
  - **生成指标**：如准确率（Accuracy）、F1分数，特别在多跳推理数据集（如HotpotQA）上使用F1分数。
  - **端到端指标**：综合评估RAG系统的性能，通常基于数据集的标准指标。
- **中文/领域支持**：
  - **语言支持**：数据集和算法主要针对英语，但通过加载中文数据集（如中文维基或医疗文献）可支持中文。
  - **领域支持**：通过选择医疗领域的数据集和算法（如医疗知识图谱）进行适配。
- **简要评述**：FlashRAG 提供了丰富的RAG算法和数据集资源，适合进行RAG研究的复现和开发。其多样化的算法支持和灵活的管道组装为中文医疗RAG-bench提供了借鉴价值，但需要额外的工作来集成中文数据集和模型，以满足医疗领域的专业需求。相比之下，其多模态支持可能为处理医疗图像数据提供额外可能性。

### open-rag-eval

- **一句话总结**：open-rag-eval 是一个开源Python评估工具包，用于评估检索增强生成（RAG）管道，提供灵活的框架和不需要黄金答案的指标，利用UMBRELA和AutoNuggetizer等技术。
- **GitHub链接**：https://github.com/vectara/open-rag-eval
- **官方文档**：https://github.com/vectara/open-rag-eval#documentation
- **核心技术论文**：无明确核心论文，但参考TREC-RAG相关研究。
- **核心特性**：
  1. **无参考评估**：使用UMBRELA和AutoNuggetizer技术，无需黄金答案即可进行评估。
  2. **模块化架构**：支持自定义评估指标和与多种RAG管道的整合。
  3. **标准指标**：提供TREC-RAG评估指标，如精度、召回等。
  4. **连接器**：支持与Vectara、LlamaIndex和LangChain等平台的集成。
- **评估指标**：
  - **检索指标**：TREC-RAG指标，如精度（Precision）、召回（Recall）、平均倒数排名（MRR）。
  - **生成指标**：如忠实度（Hallucination，评估生成内容是否包含幻觉）、答案质量。
  - **端到端指标**：综合评估RAG系统的性能，基于无参考方法。
- **中文/领域支持**：
  - **语言支持**：作为通用框架，支持多种语言，具体取决于RAG系统的语言模型和数据。
  - **领域支持**：通过自定义指标和连接器，可适配医疗领域。
- **简要评述**：open-rag-eval 的无参考评估方法对于中文医疗RAG-bench特别有用，因为医疗领域标注数据通常难以获取。其模块化设计和与流行平台的集成便于在现有系统中实施，但需要配置支持中文的模型和医疗数据集以确保评估的准确性。

### giskard

- **一句话总结**：giskard 是一个开源的评估和测试框架，旨在控制AI和LLM系统的性能、偏见和安全风险，包括对RAG系统的评估。
- **GitHub链接**：https://github.com/Giskard-AI/giskard
- **官方文档**：https://docs.giskard.ai/
- **核心技术论文**：无明确核心论文，但参考 https://arxiv.org/abs/2405.07437
- **核心特性**：
  1. **自动测试集生成**：从RAG知识库自动生成问题、参考答案和参考上下文。
  2. **组件级评估**：分别评估RAG的生成器、检索器、重写器、路由器和知识库。
  3. **集成性**：与LangChain、LlamaIndex等框架无缝集成。
  4. **自定义测试**：允许用户定义特定于任务的测试和指标。
- **评估指标**：
  - **检索指标**：如上下文相关性、召回率。
  - **生成指标**：如答案正确性、连贯性。
  - **端到端指标**：综合评估RAG系统的性能，基于组件得分的聚合。
- **中文/领域支持**：
  - **语言支持**：支持多种语言，具体取决于使用的模型和数据集。
  - **领域支持**：通过加载医疗知识库和自定义测试集，可适配医疗领域。
- **简要评述**：giskard 的自动测试集生成和组件级评估功能对于中文医疗RAG-bench非常有用，可以帮助识别系统中的具体薄弱环节。其与流行框架的集成便于实施，但需要针对中文医疗领域的术语和数据隐私要求进行额外配置。

## 评估指标对比

以下表格总结了各项目的评估指标与中文医疗RAG-bench需求的对比：

| 项目名称       | 检索指标                     | 生成指标                     | 端到端指标           | 与中文医疗RAG-bench的差异 |
|----------------|-----------------------------|-----------------------------|---------------------|-------------------------|
| ragas         | 上下文精度、上下文召回       | 忠实度、答案相关性、答案正确性 | 综合性能评估         | 需配置中文模型和医疗数据集 |
| FlashRAG      | 精度、召回、MRR             | 准确率、F1分数              | 数据集特定指标       | 主要为英语，需适配中文和医疗 |
| open-rag-eval | TREC-RAG指标（精度、召回等） | 忠实度、答案质量            | 无参考综合评估       | 无参考评估适合医疗领域数据稀缺 |
| giskard       | 上下文相关性、召回率         | 答案正确性、连贯性          | 组件得分聚合         | 需定制医疗测试集和指标 |

## 中文和医疗领域支持分析

- **语言支持**：所有四个框架均为通用框架，支持多语言评估，具体效果取决于使用的语言模型和数据集。通过集成支持中文的模型（如中文BERT、LLaMA中文变体）或中文数据集（如中文维基、医学文献），可实现中文支持。
- **医疗领域支持**：这些框架未直接针对医疗领域设计，但通过加载医疗数据集（如中文电子病历、医学文献）或微调模型，可适配医疗领域。医疗领域的专业术语和数据隐私要求可能需要额外的预处理和安全措施。
- **建议**：为构建中文医疗RAG-bench，建议结合ragas的自动化测试生成和open-rag-eval的无参考评估方法，以应对医疗领域标注数据不足的问题。同时，利用giskard的组件级评估优化系统性能，参考FlashRAG的算法多样性设计复杂推理任务。

## 可借鉴的最佳实践

- **ragas**：其自动化测试数据生成和与LangChain等框架的集成可简化中文医疗RAG的评估流程，适合快速构建评估管道。
- **FlashRAG**：多样化的算法支持和灵活的管道组装为医疗领域的复杂推理任务（如多跳医疗问题）提供了灵感。
- **open-rag-eval**：无参考评估方法适合医疗领域中难以获取标注数据的情况，可降低评估成本。
- **giskard**：组件级评估和自动测试集生成有助于精准优化RAG系统的检索和生成模块，适合医疗领域的精细化需求。

## 结论

ragas、FlashRAG、open-rag-eval和giskard是当前最成熟、社区活跃的开源RAG评测框架，适合作为中文医疗RAG-bench的参考。这些框架提供了标准化的评估流程和多样化的指标，能够通过配置中文模型和医疗数据集适配特定需求。建议结合ragas的自动化测试生成、open-rag-eval的无参考评估和giskard的组件级评估，构建一个针对中文医疗领域的RAG基准测试框架，同时参考FlashRAG的算法多样性以支持复杂医疗任务。